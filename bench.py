import jinja2
import os
import sys
import subprocess
import pandas as pd
from datetime import datetime
from pathlib import Path
import re
import argparse

def list_available_unary_operations():

    import operations

    all_operations = operations.UNARY_OPERATIONS
    all_operations = [(variant_name, base_operation_name) for variant_name, base_operation_name, _, _ in operations.iterate_all_operations(all_operations)]


    return all_operations

def _find_ops_perf_results_csv(output_directory, name_append):
    """Find the ops_perf_results CSV file generated by tracy"""
    output_path = Path(output_directory).resolve()
    reports_dir = output_path / "reports"
    
    if not reports_dir.exists():
        if output_path.name == "reports":
            reports_dir = output_path
            output_path = output_path.parent
        else:
            # Check for direct ops_perf_results files in output_directory
            csv_files = list(output_path.glob("**/ops_perf_results*.csv"))
            if csv_files:
                csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                return str(csv_files[0])
            return None
    
    # Search in name_append subdirectories
    search_dirs = []
    if name_append:
        name_dir = reports_dir / name_append
        if name_dir.exists():
            timestamp_dirs = [d for d in name_dir.iterdir() if d.is_dir()]
            search_dirs.extend(timestamp_dirs)
            search_dirs.append(name_dir)
    
    # Also check timestamped directories directly in reports_dir
    timestamp_dirs = [d for d in reports_dir.iterdir() if d.is_dir()]
    search_dirs.extend(timestamp_dirs)
    search_dirs.append(reports_dir)
    
    # Search for ops_perf_results CSV files, prioritizing most recently modified
    all_csv_files = []
    for search_dir in search_dirs:
        all_csv_files.extend(search_dir.glob("ops_perf_results*.csv"))
    
    if all_csv_files:
        all_csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        return str(all_csv_files[0])
    
    # Fallback: search entire output directory recursively
    csv_files = list(output_path.glob("**/ops_perf_results*.csv"))
    if csv_files:
        csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        return str(csv_files[0])
    
    return None

def find_latest_profiler_csv(operation, implementation_name):
    """
    Finds the CSV file in the most recently created directory in
    "$TT_METAL_HOME/generated/profiler/reports/reports/"
    that matches the given operation and implementation_name pattern:
    {operation}_{implemention_name}_*/
    Returns the path to the CSV file if found, else None.
    """
    import os
    from pathlib import Path
    import glob

    tt_metal_home = os.environ.get("TT_METAL_HOME")
    if not tt_metal_home:
        raise RuntimeError("TT_METAL_HOME environment variable is not set")

    reports_dir = Path(tt_metal_home) / "generated" / "profiler" / "reports" / "reports"
    if not reports_dir.exists():
        raise FileNotFoundError(f"{reports_dir} does not exist")

    pattern = f"{operation}_{implementation_name}_*"
    dirs = [d for d in reports_dir.glob(pattern) if d.is_dir()]
    if not dirs:
        return None

    dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
    latest_dir = dirs[0]

    csv_files = list(latest_dir.glob("*.csv"))
    if not csv_files:
        return None

    # Return the first CSV file (there's typically only one per run)
    return str(csv_files[0])


def run_bench(impl_file, implementation, dtype, dest_dir):

    implementation_name, base_operation_name = implementation
    print(f"Running benchmark for {base_operation_name} {implementation_name}")

    # Create unique name for this benchmark run
    timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')
    name_append = f"{base_operation_name}_{implementation_name}_{timestamp}"
    
    BENCH_ITERATIONS = 10
    BENCH_DTYPE = dtype


    # Build tracy command
    cmd = [
        "python", "-m", "tracy",
        "-r",  # Generate ops report
        "-p",  # Partial profile
        "-o", dest_dir,
        "-n", name_append,
        impl_file,
        str(BENCH_ITERATIONS),
        BENCH_DTYPE,
        implementation_name,
    ]
    
    # Launch tracy for the given operation file
    print(f"Running tracy: {' '.join(cmd)}")
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
    except subprocess.CalledProcessError as e:
        print(f"Error running tracy: {e}")
        print(f"stderr: {e.stderr}")
        print(f"stdout: {e.stdout}")
        return pd.DataFrame() # Return empty dataframe if tracy fails

    df = pd.DataFrame()
    # Find the ops_perf_results CSV file
    try:
        csv_path = _find_ops_perf_results_csv(dest_dir, name_append)
    
        if not csv_path:
            raise RuntimeError(f"Could not find ops_perf_results CSV for {name_append} in {dest_dir}")
    
        # Read and return the CSV as a pandas dataframe
        df = pd.read_csv(csv_path)

        df["base_operation_name"] = base_operation_name
        df["implementation_name"] = implementation_name
    except Exception as e:
        print(f"Error reading CSV: {e}")
        return pd.DataFrame() # Return empty dataframe if CSV reading fails
    
    return df

def run_benchmarks(operation_file, all_implementations, dtype, dest_dir):
    
    df_all_results = pd.DataFrame()
    for implementation in all_implementations:
        print(f"Running benchmark for {implementation}")
        df = run_bench(operation_file, implementation, dtype, dest_dir)
        df_all_results = pd.concat([df_all_results, df], ignore_index=True)

    return df_all_results

def parse_input_size(size_str):
    expr = re.compile(r'(\d+)\[\d+\]')

    match = expr.search(size_str)
    if match:
        return int(match.group(1))
    else:
        raise ValueError(f"Invalid size string: {size_str}")

def process_benchmarks(df_all_results):

    TILE_SIZE = 1024

    ignore_row = True

    processed_results = pd.DataFrame()
    for _, row in df_all_results.iterrows():

        if row["OP CODE"] == "BENCHMARK START":
            ignore_row = False
            continue
        elif row["OP CODE"] == "BENCHMARK END":
            ignore_row = True
            continue

        if ignore_row:
            continue

        CORE_FREQ = 1e9 # Hardcode value for Wormhole
        SECONDS_PER_UNIT = 1e9 # Nanoseconds per s
        CYCLES_PER_NS = CORE_FREQ / SECONDS_PER_UNIT

        input_x = parse_input_size(row["OUTPUT_0_X_PAD[LOGICAL]"])
        input_y = parse_input_size(row["OUTPUT_0_Y_PAD[LOGICAL]"])
        input_z = parse_input_size(row["OUTPUT_0_Z_PAD[LOGICAL]"])
        input_w = parse_input_size(row["OUTPUT_0_W_PAD[LOGICAL]"])

        core_count = row["CORE COUNT"]

        input_size = input_x * input_y * input_z * input_w
        base_operation_name = row["base_operation_name"]
        implementation_name = row["implementation_name"]
        cycles_per_datum = row["DEVICE KERNEL DURATION PER CORE MIN [ns]"] * core_count / input_size * CYCLES_PER_NS
        cycles_per_tile = cycles_per_datum * TILE_SIZE
        processed_results = pd.concat([processed_results, pd.DataFrame({"base_operation_name": [base_operation_name], "implementation_name": [implementation_name], "cycles_per_datum": [cycles_per_datum], "cycles_per_tile": [cycles_per_tile]})], ignore_index=True)

    processed_results = processed_results.groupby(["implementation_name"], sort=False).min().reset_index()

    return processed_results

def main(args):

    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Benchmark TTNN unary operations")
    parser.add_argument(
        "--operation", "-k",
        type=str,
        default=None,
        help="Filter operations by base operation name. If specified, runs all variants of the given operation. If not specified, runs all operations."
    )
    parser.add_argument(
        "--dtype", "-t",
        type=str,
        default="bfloat16",
        help="Data type to benchmark (default: bfloat16). Must be one of: float32, bfloat16"
    )
    
    parsed_args = parser.parse_args(args)
    
    all_operations = list_available_unary_operations()
    print(f"All operations: {all_operations}")

    # Filter by base operation name if -k is specified
    if parsed_args.operation:
        filtered_operations = [
            (variant_name, base_operation_name)
            for variant_name, base_operation_name in all_operations
            if base_operation_name == parsed_args.operation
        ]
        
        if not filtered_operations:
            import operations
            available_ops = list(operations.UNARY_OPERATIONS.keys())
            print(f"Error: Operation '{parsed_args.operation}' not found.")
            print(f"Available operations: {', '.join(sorted(available_ops))}")
            sys.exit(1)
        
        all_operations = filtered_operations
        print(f"Running benchmarks for operation '{parsed_args.operation}' ({len(all_operations)} variants)")

    os.makedirs("generated/benchmarks/unary/", exist_ok=True)

    # all_implementations = generate_all_operations_templates(all_operations, "generated/benchmarks/unary/", "bench-unary.py.j2", 10, "bfloat16")
    operation_file = "templates/bench-unary.py"

    METAL_HOME = os.getenv("TT_METAL_HOME")
    benchmark_dest_dir = f"{METAL_HOME}/generated/profiler/reports/"
    df_all_results = run_benchmarks(operation_file, all_operations, parsed_args.dtype, benchmark_dest_dir)
    df_processed_results = process_benchmarks(df_all_results)

    print(f"Processed results: {df_processed_results}")
    output_csv_path = "generated/benchmarks/unary/processed_results.csv"
    if parsed_args.operation:
        output_csv_path = f"generated/benchmarks/unary/{parsed_args.operation}.csv"
    df_processed_results.to_csv(output_csv_path, index=False)

if __name__ == "__main__":
    args = sys.argv[1:]
    main(args)