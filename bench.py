import jinja2
import os
import sys
import subprocess
import pandas as pd
from datetime import datetime
from pathlib import Path
import re
import argparse

def get_freq():

    import ttnn

    arch_name = ttnn.get_arch_name()
    if arch_name == "wormhole_b0":
        return 1e9
    elif arch_name == "blackhole":
        return 1.35e9
    else:
        raise ValueError(f"Unknown arch name: {arch_name}")


def list_available_unary_operations():

    import operations

    all_operations = operations.UNARY_OPERATIONS
    all_operations = [(variant_name, base_operation_name) for variant_name, base_operation_name, _, _ in operations.iterate_all_operations(all_operations)]


    return all_operations

def list_available_binary_operations():

    import operations

    all_operations = operations.BINARY_OPERATIONS
    all_operations = [(variant_name, base_operation_name) for variant_name, base_operation_name, _, _ in operations.iterate_all_operations(all_operations)]


    return all_operations

def detect_operation_type(operation_name):
    """Detect if an operation is unary or binary"""
    import operations
    
    if operation_name in operations.UNARY_OPERATIONS:
        return "unary"
    elif operation_name in operations.BINARY_OPERATIONS:
        return "binary"
    else:
        return None

def _find_ops_perf_results_csv(output_directory, name_append):
    """Find the ops_perf_results CSV file generated by tracy"""
    output_path = Path(output_directory).resolve()
    reports_dir = output_path / "reports"
    
    if not reports_dir.exists():
        if output_path.name == "reports":
            reports_dir = output_path
            output_path = output_path.parent
        else:
            # Check for direct ops_perf_results files in output_directory
            csv_files = list(output_path.glob("**/ops_perf_results*.csv"))
            if csv_files:
                csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                return str(csv_files[0])
            return None
    
    # Search in name_append subdirectories
    search_dirs = []
    if name_append:
        name_dir = reports_dir / name_append
        if name_dir.exists():
            timestamp_dirs = [d for d in name_dir.iterdir() if d.is_dir()]
            search_dirs.extend(timestamp_dirs)
            search_dirs.append(name_dir)
    
    # Also check timestamped directories directly in reports_dir
    timestamp_dirs = [d for d in reports_dir.iterdir() if d.is_dir()]
    search_dirs.extend(timestamp_dirs)
    search_dirs.append(reports_dir)
    
    # Search for ops_perf_results CSV files, prioritizing most recently modified
    all_csv_files = []
    for search_dir in search_dirs:
        all_csv_files.extend(search_dir.glob("ops_perf_results*.csv"))
    
    if all_csv_files:
        all_csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        return str(all_csv_files[0])
    
    # Fallback: search entire output directory recursively
    csv_files = list(output_path.glob("**/ops_perf_results*.csv"))
    if csv_files:
        csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        return str(csv_files[0])
    
    return None

def find_latest_profiler_csv(operation, implementation_name):
    """
    Finds the CSV file in the most recently created directory in
    "$TT_METAL_HOME/generated/profiler/reports/reports/"
    that matches the given operation and implementation_name pattern:
    {operation}_{implemention_name}_*/
    Returns the path to the CSV file if found, else None.
    """
    import os
    from pathlib import Path
    import glob

    tt_metal_home = os.environ.get("TT_METAL_HOME")
    if not tt_metal_home:
        raise RuntimeError("TT_METAL_HOME environment variable is not set")

    reports_dir = Path(tt_metal_home) / "generated" / "profiler" / "reports" / "reports"
    if not reports_dir.exists():
        raise FileNotFoundError(f"{reports_dir} does not exist")

    pattern = f"{operation}_{implementation_name}_*"
    dirs = [d for d in reports_dir.glob(pattern) if d.is_dir()]
    if not dirs:
        return None

    dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
    latest_dir = dirs[0]

    csv_files = list(latest_dir.glob("*.csv"))
    if not csv_files:
        return None

    # Return the first CSV file (there's typically only one per run)
    return str(csv_files[0])


def run_bench(impl_file, implementation, dtype, dest_dir, operation_type="unary"):

    implementation_name, base_operation_name = implementation
    print(f"Running benchmark for {base_operation_name} {implementation_name}")

    # Create unique name for this benchmark run
    timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')
    name_append = f"{base_operation_name}_{implementation_name}_{timestamp}"
    
    BENCH_ITERATIONS = 10
    BENCH_DTYPE = dtype


    # Build tracy command
    cmd = [
        "python", "-m", "tracy",
        "-r",  # Generate ops report
        "-p",  # Partial profile
        "--no-runtime-analysis",
        "-o", dest_dir,
        "-n", name_append,
        impl_file,
        str(BENCH_ITERATIONS),
        BENCH_DTYPE,
        implementation_name,
    ]
    
    # Launch tracy for the given operation file
    print(f"Running tracy: {' '.join(cmd)}")
    subprocess_stdout = ""
    subprocess_stderr = ""
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        subprocess_stdout = result.stdout
        subprocess_stderr = result.stderr
    except subprocess.CalledProcessError as e:
        print(f"Error running tracy for {implementation_name}: {e}")
        if e.returncode == 1:
            print(f"\n{'='*80}")
            print(f"BENCHMARK ERROR for {implementation_name}")
            print(f"{'='*80}")
            if e.stderr:
                print(f"stderr:\n{e.stderr}")
            if e.stdout:
                print(f"stdout:\n{e.stdout}")
            print(f"{'='*80}\n")
        else:
            print(f"stderr: {e.stderr}")
            print(f"stdout: {e.stdout}")
        return pd.DataFrame() # Return empty dataframe if tracy fails

    df = pd.DataFrame()
    # Find the ops_perf_results CSV file
    try:
        csv_path = _find_ops_perf_results_csv(dest_dir, name_append)
    
        if not csv_path:
            raise RuntimeError(f"Could not find ops_perf_results CSV for {name_append} in {dest_dir}")
    
        # Read and return the CSV as a pandas dataframe
        df = pd.read_csv(csv_path)
        print(f"Read CSV: {csv_path}")
        
        # Check if benchmark completed successfully by looking for BENCHMARK END marker
        if "OP CODE" in df.columns:
            has_start = (df["OP CODE"] == "BENCHMARK START").any()
            has_end = (df["OP CODE"] == "BENCHMARK END").any()
            
            if has_start and not has_end:
                print(f"\n{'='*80}")
                print(f"BENCHMARK INCOMPLETE for {implementation_name}")
                print(f"{'='*80}")
                print(f"The benchmark started but did not complete (missing BENCHMARK END marker).")
                print(f"This usually means the benchmark script crashed or was interrupted.")
                print(f"\nChild process output:")
                print(f"---")
                if subprocess_stderr:
                    print(f"stderr:\n{subprocess_stderr}")
                if subprocess_stdout:
                    print(f"stdout:\n{subprocess_stdout}")
                if not subprocess_stderr and not subprocess_stdout:
                    print("(no output captured)")
                print(f"---")
                print(f"\nTo debug, run the benchmark manually:")
                print(f"  python templates/bench-{operation_type}.py {BENCH_ITERATIONS} {BENCH_DTYPE} {implementation_name}")
                print(f"{'='*80}\n")
                return pd.DataFrame() # Return empty dataframe for incomplete benchmark
        
        print(f"DF: {df}")

        df["base_operation_name"] = base_operation_name
        df["implementation_name"] = implementation_name
    except Exception as e:
        print(f"Error reading CSV: {e}")
        return pd.DataFrame() # Return empty dataframe if CSV reading fails
    
    return df

def run_benchmarks(operation_file, all_implementations, dtype, dest_dir, operation_type="unary"):
    
    df_all_results = pd.DataFrame()
    for implementation in all_implementations:
        print(f"Running benchmark for {implementation}")
        df = run_bench(operation_file, implementation, dtype, dest_dir, operation_type)
        df_all_results = pd.concat([df_all_results, df], ignore_index=True)

    return df_all_results

def parse_input_size(size_str):
    expr = re.compile(r'(\d+)\[\d+\]')

    match = expr.search(size_str)
    if match:
        return int(match.group(1))
    else:
        raise ValueError(f"Invalid size string: {size_str}")

def process_benchmarks(df_all_results):

    TILE_SIZE = 1024

    # Handle empty dataframe
    if df_all_results.empty:
        print("Warning: No benchmark results to process (empty dataframe)")
        return pd.DataFrame(columns=["implementation_name", "cycles_per_datum", "cycles_per_tile"])
    
    # Check if required columns exist
    required_cols = ["base_operation_name", "implementation_name"]
    missing_cols = [col for col in required_cols if col not in df_all_results.columns]
    if missing_cols:
        print(f"Warning: Missing required columns in dataframe: {missing_cols}")
        return pd.DataFrame(columns=["implementation_name", "cycles_per_datum", "cycles_per_tile"])

    ignore_row = True

    processed_results = pd.DataFrame()

    composite_cycles_per_datum = 0
    composite_cycles_per_tile = 0

    for _, row in df_all_results.iterrows():


        if row["OP CODE"] == "BENCHMARK START":
            ignore_row = False
            continue
        elif row["OP CODE"] == "BENCHMARK END":
            ignore_row = True
            continue

        
        if ignore_row:
            continue

        # Use to sum time of all sub-operations in composite operations
        if row["OP CODE"] == "ITERATION START":
            composite_cycles_per_datum = 0
            composite_cycles_per_tile = 0
            continue
        elif row["OP CODE"] == "ITERATION END":
            processed_results = pd.concat([
                processed_results, pd.DataFrame({
                    "base_operation_name": [base_operation_name], 
                    "implementation_name": [implementation_name], 
                    "cycles_per_datum": [composite_cycles_per_datum], 
                    "cycles_per_tile": [composite_cycles_per_tile]
                })
            ], ignore_index=True)

            composite_cycles_per_datum = 0
            composite_cycles_per_tile = 0

            continue


        CORE_FREQ = get_freq() # Hardcode value for Wormhole
        SECONDS_PER_UNIT = 1e9 # Nanoseconds per s
        CYCLES_PER_NS = CORE_FREQ / SECONDS_PER_UNIT

        input_x = parse_input_size(row["OUTPUT_0_X_PAD[LOGICAL]"])
        input_y = parse_input_size(row["OUTPUT_0_Y_PAD[LOGICAL]"])
        input_z = parse_input_size(row["OUTPUT_0_Z_PAD[LOGICAL]"])
        input_w = parse_input_size(row["OUTPUT_0_W_PAD[LOGICAL]"])

        core_count = row["CORE COUNT"]

        input_size = input_x * input_y * input_z * input_w
        base_operation_name = row["base_operation_name"]
        implementation_name = row["implementation_name"]

        composite_cycles_per_datum += row["DEVICE KERNEL DURATION PER CORE MIN [ns]"] * core_count / input_size * CYCLES_PER_NS
        composite_cycles_per_tile += composite_cycles_per_datum * TILE_SIZE
        
    # Handle case where no valid benchmark results were found
    if processed_results.empty:
        print("Warning: No valid benchmark results found to process")
        return pd.DataFrame(columns=["implementation_name", "cycles_per_datum", "cycles_per_tile"])

    processed_results = processed_results.groupby(["implementation_name"], sort=False).min().reset_index()

    return processed_results

def main(args):

    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Benchmark TTNN unary and binary operations")
    parser.add_argument(
        "--operation", "-k",
        type=str,
        default=None,
        help="Filter operations by base operation name. If specified, runs all variants of the given operation. If not specified, runs all operations."
    )
    parser.add_argument(
        "--dtype", "-t",
        type=str,
        default="bfloat16",
        help="Data type to benchmark (default: bfloat16). Must be one of: float32, bfloat16"
    )
    parser.add_argument(
        "--type",
        type=str,
        default="unary",
        choices=["unary", "binary"],
        help="Type of operations to benchmark (default: unary). Must be one of: unary, binary"
    )
    
    parsed_args = parser.parse_args(args)
    
    # Detect operation type if a specific operation is provided
    operation_type = parsed_args.type
    if parsed_args.operation:
        detected_type = detect_operation_type(parsed_args.operation)
        if detected_type is None:
            import operations
            unary_ops = list(operations.UNARY_OPERATIONS.keys())
            binary_ops = list(operations.BINARY_OPERATIONS.keys())
            print(f"Error: Operation '{parsed_args.operation}' not found.")
            print(f"Available unary operations: {', '.join(sorted(unary_ops))}")
            print(f"Available binary operations: {', '.join(sorted(binary_ops))}")
            sys.exit(1)
        operation_type = detected_type
        print(f"Detected operation type: {operation_type}")
    
    # Get operations based on type
    if operation_type == "binary":
        all_operations = list_available_binary_operations()
        operation_file = "templates/bench-binary.py"
        output_dir = "generated/benchmarks/binary/"
    else:
        all_operations = list_available_unary_operations()
        operation_file = "templates/bench-unary.py"
        output_dir = "generated/benchmarks/unary/"
    
    print(f"All {operation_type} operations: {all_operations}")

    # Filter by base operation name if -k is specified
    if parsed_args.operation:
        filtered_operations = [
            (variant_name, base_operation_name)
            for variant_name, base_operation_name in all_operations
            if base_operation_name == parsed_args.operation
        ]
        
        if not filtered_operations:
            print(f"Error: Operation '{parsed_args.operation}' not found in {operation_type} operations.")
            sys.exit(1)
        
        all_operations = filtered_operations
        print(f"Running benchmarks for operation '{parsed_args.operation}' ({len(all_operations)} variants)")

    os.makedirs(output_dir, exist_ok=True)

    METAL_HOME = os.getenv("TT_METAL_HOME")
    benchmark_dest_dir = f"{METAL_HOME}/generated/profiler/reports/"
    df_all_results = run_benchmarks(operation_file, all_operations, parsed_args.dtype, benchmark_dest_dir, operation_type)
    df_processed_results = process_benchmarks(df_all_results)

    print(f"Processed results: {df_processed_results}")
    output_csv_path = f"{output_dir}/processed_results.csv"
    if parsed_args.operation:
        output_csv_path = f"{output_dir}/{parsed_args.operation}.csv"
    df_processed_results.to_csv(output_csv_path, index=False)

if __name__ == "__main__":
    args = sys.argv[1:]
    main(args)