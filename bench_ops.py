import jinja2
import os
import sys
import subprocess
import pandas as pd
from datetime import datetime
from pathlib import Path
import re

def list_available_unary_operations():

    import operations

    all_operations = operations.UNARY_OPERATIONS
    all_operations = [(variant_name, base_operation_name) for variant_name, base_operation_name, _, _ in operations.iterate_all_operations(all_operations)]


    return all_operations


def generate_all_operations_templates(all_operations, dest_dir, template_file, ITERATIONS, DTYPE):
    """
    For each operation in all_operations

    all_operations: dict of {
        "base_operation_name": str, 
        "implementation_name": str
    }
    """

    os.makedirs(dest_dir, exist_ok=True)

    env = jinja2.Environment(
        loader=jinja2.FileSystemLoader("templates"),
        autoescape=True
    )

    template = env.get_template(template_file)

    all_impl_files = []

    for implementation_name, base_operation_name in all_operations:
        rendered_template = template.render(ITERATIONS=ITERATIONS, DTYPE=DTYPE, IMPLEMENTATION_NAME=implementation_name)

        impl_file = f"{dest_dir}/{base_operation_name}-{DTYPE}-{implementation_name}.py"
        with open(impl_file, "w") as f:
            f.write(rendered_template)
            all_impl_files += [(impl_file, base_operation_name, implementation_name)]

    return all_impl_files

def _find_ops_perf_results_csv(output_directory, name_append):
    """Find the ops_perf_results CSV file generated by tracy"""
    output_path = Path(output_directory).resolve()
    reports_dir = output_path / "reports"
    
    if not reports_dir.exists():
        if output_path.name == "reports":
            reports_dir = output_path
            output_path = output_path.parent
        else:
            # Check for direct ops_perf_results files in output_directory
            csv_files = list(output_path.glob("**/ops_perf_results*.csv"))
            if csv_files:
                csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                return str(csv_files[0])
            return None
    
    # Search in name_append subdirectories
    search_dirs = []
    if name_append:
        name_dir = reports_dir / name_append
        if name_dir.exists():
            timestamp_dirs = [d for d in name_dir.iterdir() if d.is_dir()]
            search_dirs.extend(timestamp_dirs)
            search_dirs.append(name_dir)
    
    # Also check timestamped directories directly in reports_dir
    timestamp_dirs = [d for d in reports_dir.iterdir() if d.is_dir()]
    search_dirs.extend(timestamp_dirs)
    search_dirs.append(reports_dir)
    
    # Search for ops_perf_results CSV files, prioritizing most recently modified
    all_csv_files = []
    for search_dir in search_dirs:
        all_csv_files.extend(search_dir.glob("ops_perf_results*.csv"))
    
    if all_csv_files:
        all_csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        return str(all_csv_files[0])
    
    # Fallback: search entire output directory recursively
    csv_files = list(output_path.glob("**/ops_perf_results*.csv"))
    if csv_files:
        csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        return str(csv_files[0])
    
    return None

def run_bench(implementation, dest_dir):

    impl_file, base_operation_name, implementation_name = implementation

    # Create unique name for this benchmark run
    timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')
    name_append = f"{base_operation_name}_{implementation_name}_{timestamp}"
    
    # Build tracy command
    cmd = [
        "python", "-m", "tracy",
        "-r",  # Generate ops report
        "-p",  # Partial profile
        "-o", dest_dir,
        "-n", name_append,
        impl_file
    ]
    
    # Launch tracy for the given operation file
    print(f"Running tracy: {' '.join(cmd)}")
    result = subprocess.run(cmd, check=True, capture_output=True, text=True)
    
    # Find the ops_perf_results CSV file
    csv_path = _find_ops_perf_results_csv(dest_dir, name_append)
    
    if not csv_path:
        raise RuntimeError(f"Could not find ops_perf_results CSV for {name_append} in {dest_dir}")
    
    # Read and return the CSV as a pandas dataframe
    df = pd.read_csv(csv_path)

    df["base_operation_name"] = base_operation_name
    df["implementation_name"] = implementation_name
    
    return df

def run_benchmarks(all_implementations, dest_dir):
    
    df_all_results = pd.DataFrame()
    for implementation in all_implementations:
        print(f"Running benchmark for {implementation}")
        df = run_bench(implementation, dest_dir)
        df_all_results = pd.concat([df_all_results, df], ignore_index=True)
        break

    return df_all_results

def parse_input_size(size_str):
    expr = re.compile(r'(\d+)\[\d+\]')

    match = expr.search(size_str)
    if match:
        return int(match.group(1))
    else:
        raise ValueError(f"Invalid size string: {size_str}")

def process_benchmarks(df_all_results):

    TILE_SIZE = 1024

    processed_results = pd.DataFrame()
    for index, row in df_all_results.iterrows():

        input_x = parse_input_size(row["OUTPUT_0_X_PAD[LOGICAL]"])
        input_y = parse_input_size(row["OUTPUT_0_Y_PAD[LOGICAL]"])
        input_z = parse_input_size(row["OUTPUT_0_Z_PAD[LOGICAL]"])
        input_w = parse_input_size(row["OUTPUT_0_W_PAD[LOGICAL]"])

        core_count = row["CORE COUNT"]

        input_size = input_x * input_y * input_z * input_w
        base_operation_name = row["base_operation_name"]
        implementation_name = row["implementation_name"]
        cycles_per_datum = row["DEVICE KERNEL DURATION PER CORE MIN [ns]"] * core_count/ input_size
        cycles_per_tile = cycles_per_datum * TILE_SIZE
        processed_results = pd.concat([processed_results, pd.DataFrame({"base_operation_name": [base_operation_name], "implementation_name": [implementation_name], "cycles_per_datum": [cycles_per_datum], "cycles_per_tile": [cycles_per_tile]})], ignore_index=True)

    return processed_results

def main(args):

    all_operations = list_available_unary_operations()

    os.makedirs("generated/benchmarks/unary/", exist_ok=True)

    all_implementations = generate_all_operations_templates(all_operations, "generated/benchmarks/unary/", "bench-unary.py.j2", 10, "bfloat16")

    METAL_HOME = os.getenv("TT_METAL_HOME")
    benchmark_dest_dir = f"{METAL_HOME}/generated/profiler/reports/"
    df_all_results = run_benchmarks(all_implementations, benchmark_dest_dir)
    df_processed_results = process_benchmarks(df_all_results)

    df_processed_results.to_csv("generated/benchmarks/unary/processed_results.csv", index=False)

if __name__ == "__main__":
    args = sys.argv[1:]
    main(args)